{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略\n",
      "      玩   退出   学习   发表   睡觉\n",
      "S1  0.9  0.1  0.0  0.0  0.0\n",
      "S2  0.5  0.0  0.5  0.0  0.0\n",
      "S3  0.0  0.0  0.8  0.0  0.2\n",
      "S4  0.0  0.0  0.0  0.4  0.6\n",
      "S5    0    0    0    0    0\n",
      "回报函数\n",
      "     玩 退出  学习  发表 睡觉\n",
      "S1  -1  0   0   0  0\n",
      "S2  -1  0  -2   0  0\n",
      "S3   0  0  -2   0  0\n",
      "S4   0  0   0  10  1\n",
      "S5   0  0   0   0  0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding:utf-8 -*-\n",
    "# author : zlq16\n",
    "# date   : 2018/4/2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def main():\n",
    "    S = (\"S1\", \"S2\", \"S3\", \"S4\", \"S5\")\n",
    "    A = (\"玩\", \"退出\", \"学习\", \"发表\", \"睡觉\")\n",
    "    strategy = pd.DataFrame(data=None, index=S, columns=A)\n",
    "    reward = pd.DataFrame(data=None, index=S, columns=A)\n",
    "    gama = 1\n",
    "    strategy.loc[\"S1\", :] = np.array([0.9, 0.1, 0, 0, 0])\n",
    "    strategy.loc[\"S2\", :] = np.array([0.5, 0, 0.5, 0, 0])\n",
    "    strategy.loc[\"S3\", :] = np.array([0, 0, 0.8, 0, 0.2])\n",
    "    strategy.loc[\"S4\", :] = np.array([0, 0, 0, 0.4, 0.6])\n",
    "    strategy.loc[\"S5\", :] = np.array([0, 0, 0, 0, 0])\n",
    "    print(\"策略\")\n",
    "    print(strategy)\n",
    "    reward.loc[\"S1\", :] = np.array([-1, 0,   0,  0, 0])\n",
    "    reward.loc[\"S2\", :] = np.array([-1, 0,  -2,  0, 0])\n",
    "    reward.loc[\"S3\", :] = np.array([0,  0,  -2,  0, 0])\n",
    "    reward.loc[\"S4\", :] = np.array([0,  0,   0, 10, 1])\n",
    "    reward.loc[\"S5\", :] = np.array([0, 0, 0, 0, 0])\n",
    "    print(\"回报函数\")\n",
    "    print(reward)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----oT                          "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A simple example for Reinforcement Learning using table lookup Q-learning method.\n",
    "An agent \"o\" is on the left of a 1 dimensional world, the treasure is on the rightmost location.\n",
    "Run this program and to see how the agent will improve its strategy of finding the treasure.\n",
    "\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "np.random.seed(2)  # reproducible\n",
    "\n",
    "\n",
    "N_STATES = 6   # the length of the 1 dimensional world\n",
    "ACTIONS = ['left', 'right']     # available actions\n",
    "EPSILON = 0.9   # greedy police\n",
    "ALPHA = 0.1     # learning rate\n",
    "GAMMA = 0.9    # discount factor\n",
    "MAX_EPISODES = 13   # maximum episodes\n",
    "FRESH_TIME = 0.3    # fresh time for one move\n",
    "\n",
    "\n",
    "def build_q_table(n_states, actions):\n",
    "    q=np.zeros((n_states, len(actions)))\n",
    "    table = pd.DataFrame(\n",
    "        np.zeros((n_states, len(actions))),     # q_table initial values\n",
    "        columns=actions,    # actions's name\n",
    "    )\n",
    "    # print(table)    # show table\n",
    "    return table\n",
    "\n",
    "\n",
    "def choose_action(state, q_table):\n",
    "    # This is how to choose an action\n",
    "    state_actions = q_table.iloc[state, :]\n",
    "    if (np.random.uniform() > EPSILON) or ((state_actions == 0).all()):  # act non-greedy or state-action have no value\n",
    "        action_name = np.random.choice(ACTIONS)\n",
    "    else:   # act greedy\n",
    "        action_name = state_actions.idxmax()    # replace argmax to idxmax as argmax means a different function in newer version of pandas\n",
    "    return action_name\n",
    "\n",
    "\n",
    "def get_env_feedback(S, A):\n",
    "    # This is how agent will interact with the environment\n",
    "    if A == 'right':    # move right\n",
    "        if S == N_STATES - 2:   # terminate\n",
    "            S_ = 'terminal'\n",
    "            R = 1\n",
    "        else:\n",
    "            S_ = S + 1\n",
    "            R = 0\n",
    "    else:   # move left\n",
    "        R = 0\n",
    "        if S == 0:\n",
    "            S_ = S  # reach the wall\n",
    "        else:\n",
    "            S_ = S - 1\n",
    "    return S_, R\n",
    "\n",
    "\n",
    "def update_env(S, episode, step_counter):\n",
    "    # This is how environment be updated\n",
    "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
    "    if S == 'terminal':\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(2)\n",
    "        print('\\r                                ', end='')\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(FRESH_TIME)\n",
    "\n",
    "\n",
    "def rl():\n",
    "    # main part of RL loop\n",
    "    q_table = build_q_table(N_STATES, ACTIONS)\n",
    "    for episode in range(MAX_EPISODES):\n",
    "        step_counter = 0\n",
    "        S = 0\n",
    "        is_terminated = False\n",
    "        update_env(S, episode, step_counter)\n",
    "        while not is_terminated:\n",
    "\n",
    "            A = choose_action(S, q_table)\n",
    "            S_, R = get_env_feedback(S, A)  # take action & get next state and reward\n",
    "            q_predict = q_table.loc[S, A]\n",
    "            if S_ != 'terminal':\n",
    "                q_target = R + GAMMA * q_table.iloc[S_, :].max()   # next state is not terminal\n",
    "            else:\n",
    "                q_target = R     # next state is terminal\n",
    "                is_terminated = True    # terminate this episode\n",
    "\n",
    "            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  # update\n",
    "            S = S_  # move to next state\n",
    "\n",
    "            update_env(S, episode, step_counter+1)\n",
    "            step_counter += 1\n",
    "    return q_table\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q_table = rl()\n",
    "    print('\\r\\nQ-table:\\n')\n",
    "    print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\learn_python\\rl_study\\RL_brain.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "f:\\learn_python\\rl_study\\RL_brain.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "f:\\learn_python\\rl_study\\RL_brain.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "f:\\learn_python\\rl_study\\RL_brain.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "f:\\learn_python\\rl_study\\RL_brain.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "f:\\learn_python\\rl_study\\RL_brain.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "f:\\learn_python\\rl_study\\RL_brain.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "f:\\learn_python\\rl_study\\RL_brain.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "f:\\learn_python\\rl_study\\RL_brain.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "f:\\learn_python\\rl_study\\RL_brain.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "f:\\learn_python\\rl_study\\RL_brain.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "f:\\learn_python\\rl_study\\RL_brain.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "f:\\learn_python\\rl_study\\RL_brain.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "f:\\learn_python\\rl_study\\RL_brain.py:45: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.q_table = self.q_table.append(\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\software\\ananconda3\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"d:\\software\\ananconda3\\lib\\tkinter\\__init__.py\", line 814, in callit\n",
      "    func(*args)\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_27420\\88792261.py\", line 32, in update\n",
      "    observation_, reward, done = env.step(action)\n",
      "  File \"f:\\learn_python\\rl_study\\maze_env.py\", line 96, in step\n",
      "    s = self.canvas.coords(self.rect)\n",
      "  File \"d:\\software\\ananconda3\\lib\\tkinter\\__init__.py\", line 2766, in coords\n",
      "    self.tk.call((self._w, 'coords') + args))]\n",
      "_tkinter.TclError: invalid command name \".!canvas\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reinforcement learning maze example.\n",
    "\n",
    "Red rectangle:          explorer.\n",
    "Black rectangles:       hells       [reward = -1].\n",
    "Yellow bin circle:      paradise    [reward = +1].\n",
    "All other states:       ground      [reward = 0].\n",
    "\n",
    "This script is the main part which controls the update method of this example.\n",
    "The RL is in RL_brain.py.\n",
    "\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "\"\"\"\n",
    "\n",
    "from maze_env import Maze\n",
    "from RL_brain import QLearningTable\n",
    "\n",
    "\n",
    "def update():\n",
    "    for episode in range(100):\n",
    "        # initial observation\n",
    "        observation = env.reset()\n",
    "\n",
    "        while True:\n",
    "            # fresh env\n",
    "            env.render()\n",
    "\n",
    "            # RL choose action based on observation\n",
    "            action = RL.choose_action(str(observation))\n",
    "\n",
    "            # RL take action and get next observation and reward\n",
    "            observation_, reward, done = env.step(action)\n",
    "\n",
    "            # RL learn from this transition\n",
    "            RL.learn(str(observation), action, reward, str(observation_))\n",
    "\n",
    "            # swap observation\n",
    "            observation = observation_\n",
    "\n",
    "            # break while loop when end of this episode\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # end of game\n",
    "    print('game over')\n",
    "    env.destroy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Maze()\n",
    "    RL = QLearningTable(actions=list(range(env.n_actions)))\n",
    "\n",
    "    env.after(100, update)\n",
    "    env.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
